{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ae4e7b62e3457a9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 0: Mining the web\n",
    "\n",
    "Perhaps the richest source of openly available data today is [the Web](http://www.computerhistory.org/revolution/networking/19/314)! In this lab, you'll explore some of the basic programming tools you need to scrape web data.\n",
    "\n",
    "> **Note 0.** The Vocareum platform runs in a cloud-based environment that limits what websites a program can connect to directly. Therefore, some (or possibly all) of the code below will **not** work. Therefore, we are making this notebook **optional** and are providing some solutions inline.\n",
    ">\n",
    "> **Note 1.** Even if you are using a home or local installation of Jupyter, you may encounter problems if you attempt to access a site too many times or too rapidly. That can happen if your internet service provider (ISP) or the target website detect your accesses as \"unusual\" and reject them. It's easy to imagine accidentally writing an infinite loop that tries to access a page and being seen from the other side as a malicious program. :)\n",
    ">\n",
    "> **Note 2.** The exercises below involve processing of HTML files. However, you don't need to know anything specific about HTML; you can solve (and we have solved) all of these exercises assuming only that the data is a semi-structured string, amenable to simple string manipulation and regular expression processing techniques. In Part 1 of this notebook assignment, you'll see a different method that employs the [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) module.\n",
    ">\n",
    "> **Note 3.** Following Note 2, there are some outspoken people who believe you should never use regular expressions on HTML. Your instructor finds these arguments to be overly pedantic. For an entertaining take on the subject, see [this blog post](https://blog.codinghorror.com/parsing-html-the-cthulhu-way/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8dc1f4f41590bfcc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## The Requests module\n",
    "\n",
    "Python's [Requests module](http://requests.readthedocs.io/en/latest/user/quickstart/) to download a web page.\n",
    "\n",
    "For instance, here is a code fragment to download the [Georgia Tech](http://www.gatech.edu) home page and print the first 250 characters. You might also want to [view the source](http://www.computerhope.com/issues/ch000746.htm) of Georgia Tech's home page to get a nicely formatted view, and compare its output to what you see above.\n",
    "\n",
    "> If you you are having connection or download issues, we have also provided a file containing the HTML contents from a snapshot of the site. Just change the variable, `USE_LOCAL_SNAPSHOT` to `True` to load that file instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9670461e96fd478a",
     "locked": false,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "USE_LOCAL_SNAPSHOT = True\n",
    "\n",
    "if USE_LOCAL_SNAPSHOT:\n",
    "    print(\"\\n=== Reading webpage from local file ... ===\\n\")\n",
    "    with open('gatech_edu--20190125-1143.html', 'rt') as fp:\n",
    "        webpage = fp.read()\n",
    "else:\n",
    "    print(\"\\n=== Attempting to download webpage ... ===\\n\")\n",
    "    response = requests.get('https://www.gatech.edu/')\n",
    "    webpage = response.text  # or response.content for raw bytes\n",
    "\n",
    "print(webpage[0:250]) # Prints the first hundred characters only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1.** Given the string contents of the GT home page as above (e.g., the `webpage` variable), write a function that returns a list of links (URLs) of the site's \"top stories.\"\n",
    "\n",
    "For instance, consider the front page from Saturday, January 25, 2020:\n",
    "\n",
    "![www.gatech.edu as of Sat Jan 25, 2020](./gatech_edu--20190125-1143.png)\n",
    "\n",
    "The top stories are the ones associated with the three images (\"Quantum collaborators,\" \"10 x 10 x 10 Tech,\" and \"Transfer program offers...\").  Each image links to a news story, and we want your function to return the URL of each link. If no URLs can be found, the function should return an empty list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "get_top_stories",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import re # A simple application of regular expressions should suffice for this exercise!\n",
    "\n",
    "def get_gt_top_stories(webpage_text):\n",
    "    \"\"\"Given the HTML text for the GT front page, returns a list\n",
    "    of the URLs of the top stories or an empty list if none are\n",
    "    found.\n",
    "    \"\"\"\n",
    "    ###\n",
    "    ### YOUR CODE HERE\n",
    "    ###\n",
    "    pattern = '''<a class=\"slide-link\" href=\"(?P<url>[^\"]+)\"'''\n",
    "    return re.findall(pattern, webpage_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6016c0d059ce46b6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "top_stories = get_gt_top_stories(webpage)\n",
    "\n",
    "print(\"Your claimed links to top stories:\")\n",
    "for k, url in enumerate(top_stories):\n",
    "    print(k, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-335e708786088da3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## A more complex example\n",
    "\n",
    "Go to [Yelp!](http://www.yelp.com) and look up `ramen` in `Atlanta, GA`. Take note of the URL:\n",
    "\n",
    "![Yelp! search for ramen in ATL (January 25, 2020)](./yelp-ramen-atl--20200125-1205--scroll-to-results-annotated.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9beef10625b4a87b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "This URL encodes what is known as an _HTTP \"get\"_ method (or request). It basically means a URL with two parts: a _command_ followed by one or more _arguments_. In this case, the command is everything up to and including the word `search`; the arguments are the rest, where individual arguments are separated by the `&` or `#`.\n",
    "\n",
    "> \"HTTP\" stands for \"HyperText Transport Protocol,\" which is a standardized set of communication protocols that allow _web clients_, like your web browser or your Python program, to communicate with _web servers_.\n",
    "\n",
    "In this next example, let's see how to build a \"get request\" with the `requests` module. It's pretty easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4b1c55e113ba9c09",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "url_command = 'https://yelp.com/search'\n",
    "url_args = {'find_desc': \"ramen\",\n",
    "            'find_loc': \"atlanta, ga\"}\n",
    "response = requests.get (url_command, params=url_args, timeout=60)\n",
    "\n",
    "print(\"==> Downloading from: '%s'\" % response.url) # confirm URL\n",
    "print(\"\\n==> Excerpt from this URL:\\n\\n%s\\n\" % response.text[0:10000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sample data (HTML file) from a Yelp! query.** We've pre-downloaded the results of a query for `\"fried chicken\"` in `\"atlanta, ga\"`, and stored it in a local file. The following code cell will read its contents and store them in a variable called, `yelp_fried_chicken_atl_query_html`, which the test cells will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Query page for fried chicken in Atlanta (pre-downloaded):\n",
    "sample_query_filename = \"yelp-fried_chicken-atl--20200125-1240.html\"\n",
    "with open(sample_query_filename, \"rt\") as fp:\n",
    "    yelp_fried_chicken_atl_query_html = fp.read()\n",
    "    \n",
    "# Sample:\n",
    "print(f\"=== First few characters of '{sample_query_filename}' ===\\n\")\n",
    "print(yelp_fried_chicken_atl_query_html[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.** Given a string holding the HTML contents of a Yelp query, like the one above, complete the function below so it returns the list of the names of all **non-sponsored** search results. The list should be in ascending order of the rank of the result, and should contain no more than 10 items (since a query of the form above returns, by default, the top 10 matches).\n",
    "\n",
    "> **Note 0.** The test cell uses the pre-downloaded query file from above. You may find it helpful to open that file in a web browser, view the source, and study its contents.\n",
    ">\n",
    "> **Note 1.** We are providing one possible solution, which uses elementary string processing and regular expressions. How would you have approached this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "yelp_find_item",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_biz_names(html_string):\n",
    "    # SAMPLE SOLUTION:\n",
    "    all_results_raw = html_string.split(r'\"text\":\"All Results\"')[1]\n",
    "    items_raw = all_results_raw.split('\"ranking\":')\n",
    "    top10 = [None] * 10\n",
    "    for item in items_raw:\n",
    "        match = re.match(r'^([0-9]+),\"reviewCount\":\\d+,\"name\":\"([^\"]*)\"', item)\n",
    "        if match is not None:\n",
    "            rank = int(match.groups()[0])\n",
    "            name = match.groups()[1]\n",
    "            if 1 <= rank <= 10:\n",
    "                top10[rank-1] = name\n",
    "    return top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Demo:\n",
    "find_biz_names(yelp_fried_chicken_atl_query_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "yelp_atl__test1",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell 1: `yelp_atl__test1`\n",
    "def load_query_results(filename):\n",
    "    print(f\"Loading HTML query results from {filename}...\")\n",
    "    with open(filename, \"rt\") as fp:\n",
    "        html_string = fp.read()\n",
    "    return html_string\n",
    "\n",
    "query_0 = load_query_results(\"yelp-fried_chicken-atl--20200125-1240.html\")\n",
    "your_top10_0 = find_biz_names(query_0)\n",
    "assert your_top10_0 == ['Hattie B’s Hot Chicken - Atlanta',\n",
    " 'Gus’s World Famous Fried Chicken',\n",
    " 'Roc South Cuisine',\n",
    " 'South City Kitchen Midtown',\n",
    " 'Buttermilk Kitchen',\n",
    " 'Mary Mac’s Tea Room',\n",
    " 'Rock’s Chicken &amp; Fries',\n",
    " 'Busy Bee Cafe',\n",
    " 'Joella’s Hot Chicken - Cumberland',\n",
    " 'Gus’s World Famous Fried Chicken']\n",
    "\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "yelp_atl__test2",
     "locked": true,
     "points": 0,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Test cell 2: `yelp_atl__test2`\n",
    "query_1 = load_query_results(\"yelp-ramen-atl--20200125-1205.html\")\n",
    "your_top10_1 = find_biz_names(query_1)\n",
    "assert your_top10_1 == ['JINYA Ramen Bar',\n",
    "                        'E Ramen +',\n",
    "                        'Ginya Izakaya',\n",
    "                        'JINYA Ramen Bar',\n",
    "                        'Hajime',\n",
    "                        'Okiboru Tsukemen &amp; Ramen',\n",
    "                        'Hotto Hotto Ramen &amp; Teppanyaki',\n",
    "                        'Lifting Noodles Ramen',\n",
    "                        'Tanaka Ramen',\n",
    "                        'Ton Ton']\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-79798e6a4a127482",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "One issue with the above exercises is that they treat HTML as a flat string, whereas the document is at least semi-structured. Moreover, web pages are such a common source of data today that you would expect better tools for processing them. Indeed, such tools exist! The next part of this assignment, Part 1, walks you through one such tool. So, head there when you are ready!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": [],
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 [3.7]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
